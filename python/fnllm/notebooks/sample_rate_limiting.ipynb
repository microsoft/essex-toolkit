{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rate limiting\n",
    "\n",
    "Sample notebook that showcases rate limiting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Microsoft Corporation.\n",
    "\n",
    "import asyncio\n",
    "import random\n",
    "import string\n",
    "from collections.abc import Iterable\n",
    "from threading import Thread\n",
    "from unittest.mock import AsyncMock\n",
    "\n",
    "import tiktoken\n",
    "from fnllm import LLMUsageTracker\n",
    "from fnllm.openai import AzureOpenAIConfig, create_openai_chat_llm\n",
    "from fnllm.openai.types import (\n",
    "    OpenAIChatCompletionMessageModel,\n",
    "    OpenAIChatCompletionMessageParam,\n",
    "    OpenAIChatCompletionModel,\n",
    "    OpenAIChoiceModel,\n",
    "    OpenAICompletionUsageModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To showcase the rate limiting we are **mocking the LLM response**, always **echoing back as response what was sent as request**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_echo_response(sleep_range: tuple[int, int]):\n",
    "    async def _echo_response_wrapper(\n",
    "        messages: Iterable[OpenAIChatCompletionMessageParam], **kwargs\n",
    "    ) -> OpenAIChatCompletionModel:\n",
    "        content = str(list(messages)[-1].get(\"content\", \"No result\"))\n",
    "        n_tokens = len(tiktoken.get_encoding(\"cl100k_base\").encode(content))\n",
    "\n",
    "        await asyncio.sleep(random.randrange(*sleep_range))  # noqa: S311\n",
    "\n",
    "        return OpenAIChatCompletionModel(\n",
    "            id=\"completion_id\",\n",
    "            choices=[\n",
    "                OpenAIChoiceModel(\n",
    "                    finish_reason=\"stop\",\n",
    "                    index=0,\n",
    "                    message=OpenAIChatCompletionMessageModel(\n",
    "                        content=content, role=\"assistant\"\n",
    "                    ),\n",
    "                )\n",
    "            ],\n",
    "            created=0,\n",
    "            model=kwargs.get(\"model\", \"\"),\n",
    "            object=\"chat.completion\",\n",
    "            usage=OpenAICompletionUsageModel(\n",
    "                completion_tokens=n_tokens,\n",
    "                prompt_tokens=n_tokens,\n",
    "                total_tokens=2 * n_tokens,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    return _echo_response_wrapper\n",
    "\n",
    "\n",
    "def _mock_echo_client(sleep_range: tuple[int, int]):\n",
    "    mock = AsyncMock()\n",
    "    mock.chat.completions.create = _create_echo_response(sleep_range)\n",
    "    return mock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a printer thread that prints the `LLMUsageTracker` information periodically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _UsagePrinter:\n",
    "    def __init__(self, update_interval: int, tracker: LLMUsageTracker):\n",
    "        self._update_interval = update_interval\n",
    "        self._tracker = tracker\n",
    "        self._run_timer = False\n",
    "        self._thread: Thread | None = None\n",
    "\n",
    "    async def run(self):\n",
    "        while self._run_timer:\n",
    "            await asyncio.sleep(self._update_interval)\n",
    "            print(\n",
    "                f\"rpm={await self._tracker.current_rpm():.2f}, tpm={await self._tracker.current_tpm():.2f}, avg_rpm={await self._tracker.avg_rpm():.2f}, avg_tpm={await self._tracker.avg_tpm():.2f}, requests={self._tracker.total_requests:.2f}, usage={self._tracker.total_usage.total_tokens:.2f}, concurrency={self._tracker.current_concurrency:.2f}, max_concurrency={self._tracker.max_concurrency:.2f}\"\n",
    "            )\n",
    "\n",
    "    def start(self):\n",
    "        if not self._thread:\n",
    "            self._run_timer = True\n",
    "            self._thread = Thread(target=asyncio.run, args=(self.run(),))\n",
    "            self._thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        if self._thread:\n",
    "            self._run_timer = False\n",
    "            self._thread.join()\n",
    "\n",
    "\n",
    "def _random_str(size: int) -> str:\n",
    "    letters = string.ascii_lowercase + \" \"\n",
    "    return \"\".join(random.choice(letters) for _ in range(size))  # noqa: S311"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameters\n",
    "N_INPUTS = 100\n",
    "INPUT_SIZE_RANGE = (1000, 5000)\n",
    "\n",
    "# response parameters\n",
    "SLEEP_RANGE = (0, 5)\n",
    "\n",
    "# tracking parameters\n",
    "UPDATE_INTERVAL = 5\n",
    "\n",
    "# limiting parameters\n",
    "RPM = 30\n",
    "TPM = 80000\n",
    "MAX_CONCURRENCY = 25\n",
    "BURST_MODE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the inputs, llm and printer thread according to the configured parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [_random_str(random.randint(*INPUT_SIZE_RANGE)) for i in range(N_INPUTS)]  # noqa: S311\n",
    "\n",
    "tracker = LLMUsageTracker.create()\n",
    "\n",
    "llm = create_openai_chat_llm(\n",
    "    config=AzureOpenAIConfig(\n",
    "        endpoint=\"\",\n",
    "        api_version=\"\",\n",
    "        model=\"\",\n",
    "        requests_per_minute=RPM,\n",
    "        tokens_per_minute=TPM,\n",
    "        max_concurrency=MAX_CONCURRENCY,\n",
    "        requests_burst_mode=BURST_MODE,\n",
    "    ),\n",
    "    client=_mock_echo_client(SLEEP_RANGE),\n",
    "    events=tracker,\n",
    ")\n",
    "\n",
    "printer = _UsagePrinter(UPDATE_INTERVAL, tracker)\n",
    "\n",
    "printer.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the LLM for all the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rpm=32.00, tpm=85581.00, avg_rpm=32.00, avg_tpm=85581.00, requests=30.00, usage=88474.00, concurrency=2.00, max_concurrency=25.00\n",
      "rpm=34.00, tpm=92126.00, avg_rpm=34.00, avg_tpm=92126.00, requests=32.00, usage=95276.00, concurrency=2.00, max_concurrency=25.00\n",
      "rpm=36.00, tpm=98839.00, avg_rpm=36.00, avg_tpm=98839.00, requests=36.00, usage=105854.00, concurrency=0.00, max_concurrency=25.00\n",
      "rpm=39.00, tpm=104470.00, avg_rpm=39.00, avg_tpm=104470.00, requests=37.00, usage=109274.00, concurrency=2.00, max_concurrency=25.00\n",
      "rpm=41.00, tpm=111343.00, avg_rpm=41.00, avg_tpm=111343.00, requests=40.00, usage=114770.00, concurrency=1.00, max_concurrency=25.00\n",
      "rpm=43.00, tpm=118312.00, avg_rpm=43.00, avg_tpm=118312.00, requests=42.00, usage=121306.00, concurrency=1.00, max_concurrency=25.00\n",
      "rpm=45.00, tpm=126318.00, avg_rpm=45.00, avg_tpm=126318.00, requests=45.00, usage=128516.00, concurrency=0.00, max_concurrency=25.00\n",
      "rpm=47.00, tpm=132683.00, avg_rpm=47.00, avg_tpm=132683.00, requests=45.00, usage=128516.00, concurrency=2.00, max_concurrency=25.00\n",
      "rpm=49.00, tpm=137834.00, avg_rpm=49.00, avg_tpm=137834.00, requests=48.00, usage=140038.00, concurrency=1.00, max_concurrency=25.00\n",
      "rpm=52.00, tpm=145316.00, avg_rpm=52.00, avg_tpm=145316.00, requests=51.00, usage=152692.00, concurrency=1.00, max_concurrency=25.00\n",
      "rpm=54.00, tpm=152250.00, avg_rpm=54.00, avg_tpm=152250.00, requests=53.00, usage=157498.00, concurrency=1.00, max_concurrency=25.00\n",
      "rpm=36.00, tpm=129403.00, avg_rpm=57.00, avg_tpm=159748.00, requests=56.00, usage=166422.00, concurrency=1.00, max_concurrency=25.00\n",
      "rpm=28.00, tpm=79667.00, avg_rpm=56.21, avg_tpm=154805.21, requests=57.00, usage=169352.00, concurrency=3.00, max_concurrency=25.00\n",
      "rpm=27.00, tpm=80960.00, avg_rpm=53.79, avg_tpm=148463.87, requests=61.00, usage=179630.00, concurrency=0.00, max_concurrency=25.00\n",
      "rpm=27.00, tpm=80183.00, avg_rpm=52.46, avg_tpm=143566.54, requests=62.00, usage=182584.00, concurrency=1.00, max_concurrency=25.00\n",
      "rpm=25.00, tpm=79251.00, avg_rpm=49.22, avg_tpm=139951.34, requests=63.00, usage=184944.00, concurrency=2.00, max_concurrency=25.00\n",
      "rpm=26.00, tpm=79719.00, avg_rpm=49.34, avg_tpm=137455.43, requests=67.00, usage=195500.00, concurrency=0.00, max_concurrency=25.00\n",
      "rpm=26.00, tpm=79685.00, avg_rpm=46.54, avg_tpm=133537.31, requests=68.00, usage=198686.00, concurrency=1.00, max_concurrency=25.00\n",
      "rpm=27.00, tpm=79504.00, avg_rpm=45.62, avg_tpm=130403.26, requests=71.00, usage=206266.00, concurrency=1.00, max_concurrency=25.00\n",
      "rpm=27.00, tpm=78355.00, avg_rpm=44.99, avg_tpm=128308.88, requests=73.00, usage=212504.00, concurrency=1.00, max_concurrency=25.00\n",
      "rpm=28.00, tpm=80584.00, avg_rpm=44.84, avg_tpm=125916.34, requests=76.00, usage=220434.00, concurrency=1.00, max_concurrency=25.00\n",
      "rpm=27.00, tpm=78889.00, avg_rpm=44.14, avg_tpm=124123.84, requests=79.00, usage=233134.00, concurrency=0.00, max_concurrency=25.00\n",
      "rpm=27.00, tpm=79757.00, avg_rpm=42.93, avg_tpm=121752.84, requests=81.00, usage=238656.00, concurrency=0.00, max_concurrency=25.00\n",
      "rpm=27.00, tpm=79140.00, avg_rpm=42.26, avg_tpm=120192.76, requests=83.00, usage=247680.00, concurrency=1.00, max_concurrency=25.00\n",
      "rpm=27.00, tpm=80268.00, avg_rpm=42.07, avg_tpm=118294.12, requests=87.00, usage=258314.00, concurrency=0.00, max_concurrency=25.00\n",
      "rpm=27.00, tpm=80056.00, avg_rpm=41.13, avg_tpm=116951.38, requests=88.00, usage=262272.00, concurrency=0.00, max_concurrency=25.00\n",
      "rpm=28.00, tpm=79746.00, avg_rpm=40.85, avg_tpm=115782.99, requests=90.00, usage=266216.00, concurrency=1.00, max_concurrency=25.00\n",
      "rpm=29.00, tpm=80466.00, avg_rpm=40.53, avg_tpm=114384.29, requests=92.00, usage=272704.00, concurrency=2.00, max_concurrency=25.00\n",
      "rpm=28.00, tpm=81913.00, avg_rpm=39.38, avg_tpm=113144.53, requests=95.00, usage=284148.00, concurrency=0.00, max_concurrency=25.00\n",
      "rpm=26.00, tpm=81323.00, avg_rpm=39.38, avg_tpm=111980.28, requests=95.00, usage=284148.00, concurrency=0.00, max_concurrency=25.00\n",
      "rpm=25.00, tpm=80416.00, avg_rpm=37.98, avg_tpm=110904.35, requests=97.00, usage=293628.00, concurrency=0.00, max_concurrency=25.00\n",
      "rpm=24.00, tpm=80204.00, avg_rpm=37.07, avg_tpm=110171.15, requests=97.00, usage=293628.00, concurrency=1.00, max_concurrency=25.00\n",
      "rpm=23.00, tpm=79716.00, avg_rpm=37.02, avg_tpm=109198.78, requests=100.00, usage=308072.00, concurrency=0.00, max_concurrency=25.00\n",
      "rpm=21.00, tpm=78949.00, avg_rpm=37.02, avg_tpm=108186.09, requests=100.00, usage=308072.00, concurrency=0.00, max_concurrency=25.00\n",
      "rpm=19.00, tpm=76065.00, avg_rpm=37.02, avg_tpm=107859.76, requests=100.00, usage=308072.00, concurrency=0.00, max_concurrency=25.00\n"
     ]
    }
   ],
   "source": [
    "result = await asyncio.gather(*(llm(entry) for entry in inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "printer.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
