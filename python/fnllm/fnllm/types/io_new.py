# Copyright (c) 2025 Microsoft Corporation.
# Licensed under the MIT License

"""Primary LLM I/O types."""

from collections.abc import AsyncIterator, Sequence
from enum import StrEnum
from typing import Any, ClassVar, Generic, Protocol

from pydantic import BaseModel, ConfigDict, Field
from typing_extensions import NotRequired, TypedDict, TypeVar

from fnllm.tools.base import LLMTool
from fnllm.types.metrics import LLMMetrics

TChatMessage = TypeVar("TChatMessage", default=dict[str, str])
TModelParameters = TypeVar("TModelParameters", default=dict[str, Any])
TRawResponse = TypeVar("TRawResponse", default=Any)
TRawChatResponse = TypeVar("TRawChatResponse", default=Any)
TRawStreamingChatResponse = TypeVar("TRawStreamingChatResponse", default=Any)
TRawEmbeddingResponse = TypeVar("TRawEmbeddingResponse", default=Any)


class CacheInstruction(StrEnum):
    """Enum for cache instructions."""

    NONE = "none"
    """Bypass caching altogether, do not read or write to the cache."""

    CACHE = "cache"
    """Cache the response, read from cache if available, write cache results."""

    BUST = "bust"
    """Bust the cache, ignore existing cache entries during read, but write cache results."""


class BaseModelOptions(TypedDict, Generic[TModelParameters]):
    """Base class for model options across all LLMs (chat and embedding)."""

    name: NotRequired[str]
    """The name of the invocation for debug."""

    model_parameters: NotRequired[TModelParameters]
    """Additional model parameters to use in the invocation."""

    cache_instruction: NotRequired[CacheInstruction]
    """Instruction for cache handling. Defaults to CACHE, which reads from cache if available and writes cache results."""

    cache_metadata: NotRequired[dict[str, Any]]
    """Metadata to use when writing to the cache. This is for diagnostic/debugging purposes."""


class ChatOptions(BaseModelOptions[TModelParameters], Generic[TModelParameters]):
    """The input of a chat invocation."""

    json_model: NotRequired[type[BaseModel]] | None
    """If provided, will validate the chat response against this model."""

    tools: NotRequired[Sequence[type[LLMTool]]]
    """Tools to make available to the model. These are classes that implement LLMTool."""


class EmbeddingOptions(BaseModelOptions[TModelParameters], Generic[TModelParameters]):
    """The input of a chat invocation."""


class BaseModelResponse(BaseModel, Generic[TRawResponse]):
    """Base class for model responses across all LLMs (chat and embedding)."""

    model_config: ClassVar[ConfigDict] = ConfigDict(arbitrary_types_allowed=True)

    raw_response: TRawResponse = Field(
        description="Raw response object from the model."
    )
    usage: LLMMetrics = Field(
        default_factory=LLMMetrics,
        description="Metrics collected during the model invocation.",
    )
    cache_hit: bool | None = Field(
        default=None, description="Indicates if the response was served from cache."
    )


class ChatResponse(
    BaseModelResponse[TRawChatResponse], Generic[TChatMessage, TRawChatResponse]
):
    """Response from a chat model."""

    model_config: ClassVar[ConfigDict] = ConfigDict(arbitrary_types_allowed=True)

    message: TChatMessage = Field(
        description="The chat message generated by the model, typically a dictionary with 'role' and 'content' keys."
    )
    output_string: str | None = Field(
        default=None, description="The output string from the message."
    )
    json_model: Any | None = Field(
        default=None, description="Parsed JSON output from the model."
    )
    tool_calls: list[LLMTool] | None = Field(
        default_factory=list, description="List of tool calls made during the chat."
    )


class EmbeddingResponse(
    BaseModelResponse[TRawEmbeddingResponse], Generic[TRawEmbeddingResponse]
):
    """Response from an embedding model."""

    embeddings: list[list[float]] = Field(
        description="The generated embedding vectors."
    )


class StreamingChatResponseChunk(
    BaseModelResponse[TRawStreamingChatResponse], Generic[TRawStreamingChatResponse]
):
    """A chunk of a streaming chat response."""

    output_string: str | None = Field(
        default=None, description="The output string from the chunk."
    )
    tool_calls: list[LLMTool] | None = Field(
        default_factory=list, description="List of tool calls made during the chat."
    )
    metrics: LLMMetrics | None = Field(
        default_factory=LLMMetrics, description="Metrics collected during the chat."
    )


class ChatModel(
    Protocol,
    Generic[
        TChatMessage,
        TRawChatResponse,
        TModelParameters,
    ],
):
    """Protocol for a chat model that can generate chat completions."""

    async def chat(
        self,
        messages: list[TChatMessage],
        **kwargs: ChatOptions[TModelParameters],
    ) -> ChatResponse[TChatMessage, TRawChatResponse]:
        """Generate a chat completion based on the provided messages."""
        ...


class StreamingChatModel(
    Protocol,
    Generic[
        TChatMessage,
        TRawStreamingChatResponse,
        TModelParameters,
    ],
):
    """Protocol for a chat model that can generate chat completions as a stream."""

    async def chat_stream(
        self,
        messages: list[TChatMessage],
        **kwargs: ChatOptions[TModelParameters],
    ) -> AsyncIterator[StreamingChatResponseChunk[TRawStreamingChatResponse]]:
        """Generate a chat completion as a stream of responses."""
        ...


class EmbeddingModel(Protocol, Generic[TModelParameters]):
    """Protocol for an embedding model that can generate embeddings."""

    async def embed(
        self,
        input: list[str],
        **kwargs: EmbeddingOptions[TModelParameters],
    ) -> EmbeddingResponse:
        """Generate an embedding based on the provided input."""
        ...
