# Copyright (c) 2025 Microsoft Corporation.
# Licensed under the MIT License

"""Primary LLM I/O types."""

from collections.abc import AsyncIterator, Sequence
from enum import StrEnum
from typing import Any, Generic, Protocol

from pydantic import BaseModel, ConfigDict, Field
from typing_extensions import NotRequired, TypedDict, TypeVar

from fnllm.tools.base import LLMTool
from fnllm.types.metrics import LLMMetrics

TChatMessage = TypeVar("ChatMessage", default=dict[str, str])
TKwargs = TypeVar("TKwargs", default=dict[str, Any])
TModelParameters = TypeVar("TModelParameters", default=dict[str, Any])
TRawResponse = TypeVar("TRawResponse", Any)
TRawChatResponse = TypeVar("TRawChatResponse", Any)
TRawStreamingChatResponse = TypeVar("TRawStreamingChatResponse", default=Any)
TRawEmbeddingResponse = TypeVar("TRawEmbeddingResponse", default=Any)


class CacheInstruction(StrEnum):
    """Enum for cache instructions."""

    NONE = "none"
    """Bypass caching altogether, do not read or write to the cache."""

    CACHE = "cache"
    """Cache the response, read from cache if available, write cache results."""

    BUST = "bust"
    """Bust the cache, ignore existing cache entries during read, but write cache results."""


class ChatInput(TypedDict):
    """The input of a chat invocation."""

    name: NotRequired[str]

    """The name of the chat invocation, if available."""
    json_model: NotRequired[type[BaseModel]] | None
    """If provided, will validate the chat response against this model."""

    tools: NotRequired[Sequence[type[LLMTool]]]
    """Tools to make available to the model. These are classes that implement LLMTool."""

    cache_instruction: NotRequired[CacheInstruction]
    """Instruction for cache handling. Defaults to CACHE, which reads from cache if available and writes cache results."""

    cache_metadata: NotRequired[dict[str, Any]]
    """Metadata to use when writing to the cache. This is for diagnostic/debugging purposes."""


class ModelResponse(BaseModel, Generic[TRawResponse]):
    """Base class for model responses."""

    model_config = ConfigDict(arbitrary_types_allowed=True)

    raw_response: TRawResponse = Field(
        description="Raw response object from the model."
    )
    usage: LLMMetrics = Field(
        default_factory=LLMMetrics,
        description="Metrics collected during the model invocation.",
    )
    cache_hit: bool | None = Field(
        default=None, description="Indicates if the response was served from cache."
    )


class ChatResponse(
    ModelResponse[TRawChatResponse], Generic[TChatMessage, TRawChatResponse]
):
    """Response from a chat model."""

    model_config = ConfigDict(arbitrary_types_allowed=True)

    message: TChatMessage = Field(
        description="The chat message generated by the model, typically a dictionary with 'role' and 'content' keys."
    )
    output_string: str | None = Field(
        default=None, description="The output string from the message."
    )
    json_model: Any | None = Field(
        default=None, description="Parsed JSON output from the model."
    )
    tool_calls: list[LLMTool] | None = Field(
        default_factory=list, description="List of tool calls made during the chat."
    )


class EmbeddingResponse(ModelResponse[TRawEmbeddingResponse]):
    """Response from an embedding model."""

    embeddings: list[list[float]] = Field(
        description="The generated embedding vectors."
    )


class StreamingChatResponseChunk(ModelResponse[TRawStreamingChatResponse]):
    """A chunk of a streaming chat response."""

    output_string: str | None = Field(
        default=None, description="The output string from the chunk."
    )
    tool_calls: list[LLMTool] | None = Field(
        default_factory=list, description="List of tool calls made during the chat."
    )
    metrics: LLMMetrics | None = Field(
        default_factory=LLMMetrics, description="Metrics collected during the chat."
    )


class ChatModel(
    Protocol,
    Generic[
        TChatMessage,
        TRawChatResponse,
        TModelParameters,
        TKwargs,
    ],
):
    """Protocol for a chat model that can generate chat completions."""

    async def chat(
        self,
        messages: list[TChatMessage],
        model_parameters: TModelParameters | None = None,
        **kwargs: TKwargs,
    ) -> ChatResponse[TChatMessage, TRawChatResponse]:
        """Generate a chat completion based on the provided messages."""


class StreamingChatModel(
    Protocol,
    Generic[
        TChatMessage,
        TRawStreamingChatResponse,
        TModelParameters,
        TKwargs,
    ],
):
    """Protocol for a chat model that can generate chat completions as a stream."""

    async def chat_stream(
        self,
        messages: list[TChatMessage],
        model_parameters: TModelParameters | None = None,
        **kwargs: TKwargs,
    ) -> AsyncIterator[StreamingChatResponseChunk[TRawStreamingChatResponse]]:
        """Generate a chat completion as a stream of responses."""


class EmbeddingModel(Protocol, Generic[TModelParameters]):
    """Protocol for an embedding model that can generate embeddings."""

    async def embed(
        self,
        input: list[str],
        model_parameters: TModelParameters | None = None,
        **kwargs: TKwargs,
    ) -> EmbeddingResponse:
        """Generate an embedding based on the provided input."""
